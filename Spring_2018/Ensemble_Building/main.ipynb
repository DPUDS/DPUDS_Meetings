{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ensembles\n",
    "\n",
    "- [Check list](#Check-list)\n",
    "- [Brief intro](#Brief-intro)\n",
    "- [Problem summary](#Problem-summary)\n",
    "- [Python implementation](#Python-implementation)\n",
    "- [Takeaways](#Takeaways)\n",
    "\n",
    "# Check list\n",
    "Please make sure you have Python 3 installed, with the following libraries:\n",
    "- Pandas\n",
    "- NumPy\n",
    "- Jupyter Lab/Notebook (optional)\n",
    "\n",
    "If you want to closely follow the lecture, please register for an account on [Kaggle](http://kaggle.com/) as well, as we will be participating in a data science competition on that website.\n",
    "\n",
    "# Brief intro\n",
    "Ensembles have rapidly become one of the hottest and most popular methods in applied machine learning. Virtually every winning Kaggle solution features them, and many data science pipelines have ensembles in them.\n",
    "\n",
    "Put simply, ensembles combine predictions from different models to generate a final prediction, and the more models we include the better it performs. Better still, because ensembles combine baseline predictions, they perform at least as well as the best baseline model. Ensembles give us a performance boost almost for free.\n",
    "\n",
    "<img width=\"800\" src=\"https://www.dataquest.io/blog/content/images/2018/01/network-1.png\"></img>\n",
    "\n",
    "_Example schematics of an ensemble._\n",
    "\n",
    "_An input array X is fed through two preprocessing pipelines and then to a set of base learners f(i)._\n",
    "\n",
    "_The ensemble combines all base learner predictions into a final prediction array P._\n",
    "\n",
    "\n",
    "# Problem summary\n",
    "We will be working with the dataset in [Kaggle's DonorsChoose.org Application Screening competition](https://www.kaggle.com/c/donorschoose-application-screening).\n",
    "\n",
    "The goal of the competition is to predict whether or not a DonorsChoose.org project proposal submitted by a teacher will be approved, using the text of project descriptions as well as additional metadata about the project, teacher, and school. DonorsChoose.org can then use this information to identify projects most likely to need further review before approval.\n",
    "\n",
    "To make a submission, after training our model, we will look at individual project proposal in the testing set and predict the probability that that specific proposal will be accepted. Submissions are evaluated on [area under the ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target (true value). If you are unfamiliar with the concept, the larger the area under the ROC is, the more accurate our prediction is, and the perfect prediction will yield the score of 1.\n",
    "\n",
    "# Python implementation\n",
    "Here we are provided with premade submissions, computed from different approaches and models. We will learn how to combine these submissions to make an ensemble with a better accuracy.\n",
    "\n",
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our premade submissions are stored in the `input` folder. The individual files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgb.csv', 'nlp.csv', 'nn.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions = [file for file in os.listdir('input/') if file[-4 :] == '.csv']\n",
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the first few lines of these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb.csv\n",
      "        id  project_is_approved\n",
      "0  p233245               0.9670\n",
      "1  p096795               0.9316\n",
      "2  p236235               0.9443\n",
      "3  p233680               0.9336\n",
      "4  p171879               0.8240\n",
      "------------------------------\n",
      "nlp.csv\n",
      "        id  project_is_approved\n",
      "0  p233245             0.638546\n",
      "1  p096795             0.780585\n",
      "2  p236235             0.597326\n",
      "3  p233680             0.456532\n",
      "4  p171879             0.637596\n",
      "------------------------------\n",
      "nn.csv\n",
      "        id  project_is_approved\n",
      "0  p233245             0.929612\n",
      "1  p096795             0.916068\n",
      "2  p236235             0.971921\n",
      "3  p233680             0.890154\n",
      "4  p171879             0.883831\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for file in submissions:\n",
    "    df = pd.read_csv('input/' + file)\n",
    "    print(file)\n",
    "    print(df.head())\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `lgb.csv` is a submission made from a LightGBM model.\n",
    "- `nlp.csv` is a submission made from a pure Natural Language Processing approach.\n",
    "- `nn.csv` is a submission made from a Neural Network model.\n",
    "\n",
    "After submitting these files individual, we see that their individual score is:\n",
    "- `lgb.csv`: 0.79554\n",
    "- `nlp.csv`: 0.7959\n",
    "- `nn.csv`: 0.80016\n",
    "\n",
    "We see that individually all our submissions do quite well, but now we will, again, try to combine them together to get a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged predictions\n",
    "One intuition for this method is that, if there are proposals that some of our models do really well, while the other models don't, then averaging all predictions would decrease the inaccuracy in the bad models; if all of our models do well or badly on some proposals, the averaged prediction will most likely not stray too far from the original predictions.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.967 ,  0.9316,  0.9443, ...,  0.862 ,  0.951 ,  0.6743]),\n",
       " array([ 0.63854617,  0.78058499,  0.59732588, ...,  0.53082335,\n",
       "         0.8409909 ,  0.10038701]),\n",
       " array([ 0.9296123 ,  0.91606808,  0.97192119, ...,  0.93095801,\n",
       "         0.97309926,  0.63227701])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_values = []\n",
    "\n",
    "for file in submissions:\n",
    "    df = pd.read_csv('input/' + file)\n",
    "    submission_values.append(df['project_is_approved'].values) # 'project_is_approved is the column we need to predict\n",
    "\n",
    "submission_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84505282,  0.87608436,  0.83784902, ...,  0.77459379,\n",
       "        0.92169672,  0.468988  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_value = np.mean(submission_values, axis=0)\n",
    "avg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>0.845053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p096795</td>\n",
       "      <td>0.876084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p236235</td>\n",
       "      <td>0.837849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p233680</td>\n",
       "      <td>0.760095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p171879</td>\n",
       "      <td>0.781809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  project_is_approved\n",
       "0  p233245             0.845053\n",
       "1  p096795             0.876084\n",
       "2  p236235             0.837849\n",
       "3  p233680             0.760095\n",
       "4  p171879             0.781809"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df['id'] = df['id']\n",
    "sub_df['project_is_approved'] = avg_value\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('output/avg_sub.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting the file, we see that our score has improved quite a bit (0.81676).\n",
    "\n",
    "## Weighted predictions\n",
    "Consider our averaged ensemble. If one of our models does exceptionally well overall, taking the average of the predictions will actually negatively affect our accuracy, since the good predictions are being mixed with not-so-good ones. So we need to have a way to \"reward\" the models that good significantly better than others.\n",
    "\n",
    "One way to do this is to give those models larger weights in a weighted ensemble. Specifically, we will be using a simple linear weighted ensemble:\n",
    "\n",
    "$$P(x) = a~LGB(x) + b~NLP(x) + c~NN(x)$$\n",
    "\n",
    "$$with~0 \\leq a, b, c \\leq 1, a + b + c = 1$$\n",
    "\n",
    "We can see that the averaged ensemble we saw ealier is also a linear weighted ensemble, but all of its weights are equal to each other (in this case they are 1/3 = 0.333...)\n",
    "\n",
    "As we mentioned, let's try changing these weights so that the better model (in this case it's the Neural Network) has more weight in our ensemble. One way to do this is simply use the models' performance as their weights. Specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33263924,  0.33278976,  0.334571  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [0.79554, 0.7959, 0.80016]\n",
    "weights = np.array([score / sum(scores) for score in scores])\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84518508,  0.87614722,  0.83807181, ...,  0.77485915,\n",
       "        0.92178387,  0.46924796])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_value = np.sum(np.array([submission_values[i] * weights[i] for i in range(3)]), axis=0)\n",
    "\n",
    "weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>0.845185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p096795</td>\n",
       "      <td>0.876147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p236235</td>\n",
       "      <td>0.838072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p233680</td>\n",
       "      <td>0.760301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p171879</td>\n",
       "      <td>0.781984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  project_is_approved\n",
       "0  p233245             0.845185\n",
       "1  p096795             0.876147\n",
       "2  p236235             0.838072\n",
       "3  p233680             0.760301\n",
       "4  p171879             0.781984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['project_is_approved'] = weighted_value\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('output/weighted_sub.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this submission gives a better score than one from the averaged ensemble, but only by 0.00003, which is not a significant increase that we were hoping for. Why is that? It's because the weights that we computed for the models are very similar: 0.33263924, 0.33278976, 0.334571. To obtain a more different ensemble, we have to change the weights in a more drastic way.\n",
    "\n",
    "Let's manually try some different weight combinations. Based on the individual scores, we know that we should give `nn.csv` the largest weight, and `lgb.csv` the smallest weight, so we can try these combinations:\n",
    "\n",
    "- (0.2, 0.3, 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84977   ,  0.87852954,  0.85401836, ...,  0.79712601,\n",
       "        0.9290469 ,  0.48111461])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([0.2, 0.3, 0.5])\n",
    "weighted_value = np.sum(np.array([submission_values[i] * weights[i] for i in range(3)]), axis=0)\n",
    "\n",
    "weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['project_is_approved'] = weighted_value\n",
    "sub_df.to_csv('output/weighted_sub_v2.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission scores 0.81856. Now the increase in our score is much more significant. Let's try another combo:\n",
    "- (0.15, 0.3, 0.55):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84790061,  0.87775294,  0.85539942, ...,  0.80057391,\n",
       "        0.93015186,  0.47901346])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([0.15, 0.3, 0.55])\n",
    "weighted_value = np.sum(np.array([submission_values[i] * weights[i] for i in range(3)]), axis=0)\n",
    "\n",
    "weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['project_is_approved'] = weighted_value\n",
    "sub_df.to_csv('output/weighted_sub_v3.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another increase in our score!\n",
    "\n",
    "# Takeaways\n",
    "We have seen that making ensemble is an effective tool to increase the performance of individual predictive models. The main keys are:\n",
    "- To include models with different approaches so that they can correct each other's mistakes\n",
    "- To give models that perform better larger weights in the ensemble\n",
    "\n",
    "Through this notebook we know how to compute averaged and weighted ensembles; there are other, more complex ways to build an ensemble as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
