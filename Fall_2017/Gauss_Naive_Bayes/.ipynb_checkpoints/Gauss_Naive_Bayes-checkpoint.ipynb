{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayesian (Bayes) Classifier\n",
    "\n",
    "Bayesian Probability Theory is a field of statistics that has to do with conditional inference. Like many other classification models, the Naive Bayes model depends upon referencing a training dataset. However, unlike the KNN-Classifier and others, the Bayesian probability model relies on designating probabilities to all possible outcomes, assuming all features are independent (Naive) and abide by a certain distribution (usually a normal distribution). That is, each possible class is considered with a non-zero probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bayesian Probability\n",
    "\n",
    "Though the math behind Bayesian probability can sometimtes be daunting, understanding its greater meaning is relatively simple. In fact, most of us use Bayesian probability every day and do not know it. Let's look at what Bayesian probability formulae look like. **Baye's Theorem** looks like this.\n",
    "\n",
    "\\begin{align}\n",
    "P(A \\mid B) = \\frac{P(B \\cap A)}{P(B)}\n",
    "\\end{align}\n",
    "\n",
    "Where...\n",
    "\n",
    "\\begin{align}\n",
    "P(B \\cap A) = {P(B \\mid A) \\, P(A)}\n",
    "\\end{align}\n",
    "\n",
    "All this says is that the probability of **A** happening, given that **B** happens, is equal to the probability **they both happen** divided by the odds of **B** happening. So given our example data below, we know that the **P(Go outside | Rain)** is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P(Rain = R) = 0.5\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(Go Outside = G) = 0.8\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(Rain \\& Go Outside = R \\& G) = 0.1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that ...\n",
    "\n",
    "\\begin{align}\n",
    "P(R \\cap G) = 0.1\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "P(R) = 0.5\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "So we can then plug in to find that:\n",
    "\n",
    "\\begin{align}\n",
    "P(G \\mid R) = \\frac{P(R \\cap G)}{P(R)} = \\frac{0.1}{0.5} = 0.2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is really all there is to it! By figuring out the odds of something, given something else is already happening, we can get a good idea of what class each item belongs to.\n",
    "\n",
    "For example, say there are two islands: on one, the **average height of males and females is 7 feet tall** (woah). On the other, the **average height is 5 feet tall**. \n",
    "\n",
    "You are given someone's height, and asked to determine what island they are from using the information above. This person **is 6' 9''**. What island do you think they probably belong to? This is exactly how a Bayesian probability model works. Further along we will introduce some additional formulas, but **Baye's Theorem is by far the most important**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Dependencies and Organize Data\n",
    "\n",
    "As always, let's start with importing all of our dependencies. We intend to build this model from scratch, so the only modules we will use pertain to our random number generator and math-related functions. Sklearn is incorporated purely to import our sample dataset, which could just as easily have been done by importing a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import all dependencies\n",
    "from sklearn import datasets\n",
    "import random\n",
    "import pandas as pd \n",
    "from numpy import mean, std, exp, pi, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dependencies now included, let's take a look at our data. As shown below, the dataset we will be using pertains to flowers. The flowers to either the **setosa, versicolor, or virginica** species. This dataset is simple to understand and was chosen as such; the math behind the NB-Classifier is more involved than with the KNN-Classifier. Also, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import dictionary of all data attributes\n",
    "data = datasets.load_iris()\n",
    "\n",
    "#Shows description of data\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Features\n",
      "     0    1    2    3\n",
      "0  5.1  3.5  1.4  0.2\n",
      "1  4.9  3.0  1.4  0.2\n",
      "2  4.7  3.2  1.3  0.2\n",
      "3  4.6  3.1  1.5  0.2\n",
      "4  5.0  3.6  1.4  0.2\n",
      "\n",
      "Data Labels\n",
      "   0\n",
      "0  0\n",
      "1  0\n",
      "2  0\n",
      "3  0\n",
      "4  0\n"
     ]
    }
   ],
   "source": [
    "#Data features\n",
    "iris_data = list(data.data)\n",
    "\n",
    "#Data Labels\n",
    "iris_label = list(data.target)\n",
    "\n",
    "print(\"Data Features\")\n",
    "print(pd.DataFrame(iris_data[:5]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Data Labels\")\n",
    "print(pd.DataFrame(iris_label[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the data printed above, each of the four features exists as a float value, while the labels are represented with integers as proxies. The dictionary conversion of these values is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_dict = {0:'setosa',1:'versicolor',2:'virginica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with this new information, we know that above we are looking at five setosa flowers and their measurements. The description above mentions that there is no corrupt or missing data in this dataset, so we can now move forward to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preparing Training Data for Prediction\n",
    "\n",
    "Now that we have our dataset, lets separate it into training and testing samples. We will also need to manipulate the format of this data so we can use it for prediction later.\n",
    "\n",
    "#### Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Splits data into a training and testing set. A random sample of the test_set size is set\n",
    "by the test_ratio. The first input takes the entire dataset, excluding labels, the second input\n",
    "takes a columns of class labels, and the third input gives a test ratio.\n",
    "The first output is a tuple list of train data and train labels, the last two are the data and labels\n",
    "of the testing sample.\n",
    "\"\"\"\n",
    "def train_test_split(data,label,test_ratio):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "\n",
    "    test_set_num = int(round(test_ratio*len(data),0))\n",
    "    test_records = random.sample(range(len(data)), test_set_num)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i in test_records:\n",
    "            test_data.append(data[i])\n",
    "            test_label.append(label[i])\n",
    "        else:\n",
    "            train_record = (data[i],label[i])\n",
    "            train_data.append(train_record)\n",
    "\n",
    "    return train_data, test_data, test_label\n",
    "\n",
    "\"\"\"\n",
    "Separates records in the training dataset by class (label). This takes an input of a\n",
    "training dataset and outputs a dictionary of records by class.\n",
    "\"\"\"\n",
    "def classSep(train_data):\n",
    "    classDict = {}\n",
    "    for i in range(len(train_data)):\n",
    "        if train_data[i][1] not in classDict:\n",
    "            classDict[train_data[i][1]] = []\n",
    "        classDict[train_data[i][1]].append(train_data[i][0])\n",
    "\n",
    "    return classDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use a random number generator to give us a testing and training sample. The generator gives us a list of indices, corresponding to different records in our dataset. \n",
    "\n",
    "Next, our training data is separated by class in a dictionary format. All records that are setosas are put in one category, and so on. The final result for the testing set looks like this. **We will use 25% of our data as our testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [array([ 5.1,  3.5,  1.4,  0.2]), array([ 4.9,  3. ,  1.4,  0.2]), array([ 4.6,  3.1,  1.5,  0.2]), array([ 5. ,  3.6,  1.4,  0.2]), array([ 5.4,  3.9,  1.7,  0.4]), array([ 4.6,  3.4,  1.4,  0.3]), array([ 5. ,  3.4,  1.5,  0.2]), array([ 4.4,  2.9,  1.4,  0.2]), array([ 5.4,  3.7,  1.5,  0.2]), array([ 4.8,  3.4,  1.6,  0.2]), array([ 4.8,  3. ,  1.4,  0.1]), array([ 4.3,  3. ,  1.1,  0.1]), array([ 5.4,  3.9,  1.3,  0.4]), array([ 5.1,  3.5,  1.4,  0.3]), array([ 5.7,  3.8,  1.7,  0.3]), array([ 5.1,  3.7,  1.5,  0.4]), array([ 4.6,  3.6,  1. ,  0.2]), array([ 5.1,  3.3,  1.7,  0.5]), array([ 4.8,  3.4,  1.9,  0.2]), array([ 5.2,  3.5,  1.5,  0.2]), array([ 4.8,  3.1,  1.6,  0.2]), array([ 5.4,  3.4,  1.5,  0.4]), array([ 4.9,  3.1,  1.5,  0.1]), array([ 5. ,  3.2,  1.2,  0.2]), array([ 5.5,  3.5,  1.3,  0.2]), array([ 4.9,  3.1,  1.5,  0.1]), array([ 4.4,  3. ,  1.3,  0.2]), array([ 5.1,  3.4,  1.5,  0.2]), array([ 5. ,  3.5,  1.3,  0.3]), array([ 4.5,  2.3,  1.3,  0.3]), array([ 4.4,  3.2,  1.3,  0.2]), array([ 5. ,  3.5,  1.6,  0.6]), array([ 5.1,  3.8,  1.9,  0.4]), array([ 4.8,  3. ,  1.4,  0.3]), array([ 4.6,  3.2,  1.4,  0.2]), array([ 5.3,  3.7,  1.5,  0.2]), array([ 5. ,  3.3,  1.4,  0.2])], 1: [array([ 7. ,  3.2,  4.7,  1.4]), array([ 6.4,  3.2,  4.5,  1.5]), array([ 5.5,  2.3,  4. ,  1.3]), array([ 6.5,  2.8,  4.6,  1.5]), array([ 5.7,  2.8,  4.5,  1.3]), array([ 6.3,  3.3,  4.7,  1.6]), array([ 4.9,  2.4,  3.3,  1. ]), array([ 6.6,  2.9,  4.6,  1.3]), array([ 5.2,  2.7,  3.9,  1.4]), array([ 5. ,  2. ,  3.5,  1. ]), array([ 5.9,  3. ,  4.2,  1.5]), array([ 6. ,  2.2,  4. ,  1. ]), array([ 6.1,  2.9,  4.7,  1.4]), array([ 5.8,  2.7,  4.1,  1. ]), array([ 6.2,  2.2,  4.5,  1.5]), array([ 5.6,  2.5,  3.9,  1.1]), array([ 5.9,  3.2,  4.8,  1.8]), array([ 6.1,  2.8,  4. ,  1.3]), array([ 6.1,  2.8,  4.7,  1.2]), array([ 6.6,  3. ,  4.4,  1.4]), array([ 6.7,  3. ,  5. ,  1.7]), array([ 6. ,  2.9,  4.5,  1.5]), array([ 5.7,  2.6,  3.5,  1. ]), array([ 5.5,  2.4,  3.8,  1.1]), array([ 5.5,  2.4,  3.7,  1. ]), array([ 5.8,  2.7,  3.9,  1.2]), array([ 5.4,  3. ,  4.5,  1.5]), array([ 6.7,  3.1,  4.7,  1.5]), array([ 6.3,  2.3,  4.4,  1.3]), array([ 5.5,  2.5,  4. ,  1.3]), array([ 5.6,  2.7,  4.2,  1.3]), array([ 5.7,  3. ,  4.2,  1.2]), array([ 6.2,  2.9,  4.3,  1.3]), array([ 5.1,  2.5,  3. ,  1.1]), array([ 5.7,  2.8,  4.1,  1.3])], 2: [array([ 6.3,  3.3,  6. ,  2.5]), array([ 5.8,  2.7,  5.1,  1.9]), array([ 6.5,  3. ,  5.8,  2.2]), array([ 7.6,  3. ,  6.6,  2.1]), array([ 4.9,  2.5,  4.5,  1.7]), array([ 7.2,  3.6,  6.1,  2.5]), array([ 6.5,  3.2,  5.1,  2. ]), array([ 6.4,  2.7,  5.3,  1.9]), array([ 6.8,  3. ,  5.5,  2.1]), array([ 5.7,  2.5,  5. ,  2. ]), array([ 5.8,  2.8,  5.1,  2.4]), array([ 6.5,  3. ,  5.5,  1.8]), array([ 7.7,  3.8,  6.7,  2.2]), array([ 7.7,  2.6,  6.9,  2.3]), array([ 6. ,  2.2,  5. ,  1.5]), array([ 6.9,  3.2,  5.7,  2.3]), array([ 5.6,  2.8,  4.9,  2. ]), array([ 7.7,  2.8,  6.7,  2. ]), array([ 6.7,  3.3,  5.7,  2.1]), array([ 7.2,  3.2,  6. ,  1.8]), array([ 6.2,  2.8,  4.8,  1.8]), array([ 6.1,  3. ,  4.9,  1.8]), array([ 6.4,  2.8,  5.6,  2.1]), array([ 7.2,  3. ,  5.8,  1.6]), array([ 7.4,  2.8,  6.1,  1.9]), array([ 7.9,  3.8,  6.4,  2. ]), array([ 6.4,  2.8,  5.6,  2.2]), array([ 6.1,  2.6,  5.6,  1.4]), array([ 7.7,  3. ,  6.1,  2.3]), array([ 6.3,  3.4,  5.6,  2.4]), array([ 6. ,  3. ,  4.8,  1.8]), array([ 6.9,  3.1,  5.4,  2.1]), array([ 6.7,  3.1,  5.6,  2.4]), array([ 6.8,  3.2,  5.9,  2.3]), array([ 6.7,  3.3,  5.7,  2.5]), array([ 6.7,  3. ,  5.2,  2.3]), array([ 6.3,  2.5,  5. ,  1.9]), array([ 6.5,  3. ,  5.2,  2. ]), array([ 6.2,  3.4,  5.4,  2.3]), array([ 5.9,  3. ,  5.1,  1.8])]}\n"
     ]
    }
   ],
   "source": [
    "#Splits data into a train and testing set\n",
    "train_set, test_data, test_label = train_test_split(iris_data,iris_label,0.25)\n",
    "\n",
    "#Separates records class\n",
    "sepByClass = classSep(train_set)\n",
    "\n",
    "print(sepByClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know the data above is an eyesore, but note how the first number is a **0 followed by a list of data values**. The structure of the dictionary is **{label_key1: {data} , label_key2: {data} , label_key3: {data}}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summary Statistics\n",
    "\n",
    "Ok, now let's clean up that nasty looking array we see above into something we can work with. The most important things for a Bayesian model are to have a good disctribution of data for each class so that we can get the mean and standard deviation of the data. **The mean and standard deviation for each feature is all we need to complete our model**. However, these statistics need to be separated out by class.\n",
    "\n",
    "Before we do this, though, let's figure our why it is important. A normal distribution, as seen below, can be calculated with the mean and standard deviation of something. **The mean is the center, the standard deviation is how wide the curve is**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://image.ibb.co/fBpAH5/Bayes1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to get the mean and standard deviation of something, so we will gloss over that math and go straight to the coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a dictionary of the mean and standard deviation of every feature for each class. This takes a dictionary\n",
    "of separated training data, and outputs a dictionary of summary statistics (mean and stdev). For a dataset with three \n",
    "classes and four features, there would be a total of 3x4 = 12 means and stdevs, separated by class.\n",
    "\"\"\"\n",
    "def classStats(train_classDict):\n",
    "    classStats = {}\n",
    "    for i in train_classDict:\n",
    "        if i not in classStats:\n",
    "            classStats[i] = []\n",
    "        classStats[i] = [(mean(j), std(j))for j in zip(*train_classDict[i])]\n",
    "\n",
    "    return classStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we run our training data dictionary through this function, we get an output dictionary that looks like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0  4.945946  0.334983\n",
      "1  3.348649  0.324347\n",
      "2  1.454054  0.182467\n",
      "3  0.251351  0.110580\n"
     ]
    }
   ],
   "source": [
    "#Get summary statistics for each feature\n",
    "classStats = classStats(sepByClass)\n",
    "\n",
    "#DataFrame of the summary stats of the setosa class (mean, stdev)\n",
    "print(pd.DataFrame(list(classStats[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go. **For each class, the mean and standard deviation of each feature has been calculated and stored**. We are now ready to begin the predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Predicting for the Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the information we need from the testing data, let's get ready to begin predicting our testing data. For each testing record, we will calculate the Bayesian probability that record belongs to a given class. We will store this in s dictionary in the format **[(class1, probclass1), (class2, probclass2) ... ]**. First, however, we need to understand how we will gather these probablities.\n",
    "\n",
    "Baye's Theorem, though only used for single events in our first example, **can also be used for data with multiple contributing factors**. For example, the probability of going outside is impacted by the weather (rain), if you are injured, if you have homework, the time of day, what your friends are doing, and a million other things. To account for this kind of input, Baye's Theorem can take a new form. A conditional probability can be re-written as...\n",
    "\n",
    "\\begin{align}\n",
    "P(A \\mid B) = P(b_1 \\mid A) * P(b_2 \\mid A) * P(b_3 \\mid A) ... P(b_n \\mid A) * P(A)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the **true conditional probability of A given B is the product of the probability of all contributing sub-factors of B, given something else (in this case, A) is happening, multiplied by the odds A actually happens**. This can be seen to be:\n",
    "\n",
    "\\begin{align}\n",
    "P(A \\mid B) = P(A) * \\prod_{i=1}^{n}P(b_i \\mid A)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how can we get each individual probability then? Well, we can use what is called a **probability density function**. This is essentially a **normal curve**, just like the one we saw above. To get the probability we need from this distribution, we can **plug our mean, standard deviation, and mystery feature value into the following formula**. The closer the mystery value is to the mean of the distribution, the higher probability it belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}{\\sigma}}\\mathrm{e}^{{-\\frac{(x - \\mu)^2}{2\\sigma^2}}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this function does not explicitly state it give probabilities is that it will **return values that are greater than one**. For our purposes, this is not important, so we will move on to actually using this function to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the probability of a feature given this attribute follows a normal (Gaussian) Distribution\n",
    "with the provided mean and standard deviation.\n",
    "\"\"\"\n",
    "def gaussProb(mean,stddev,feature):\n",
    "    num = exp(-((feature - mean)**2) / (2*(stddev**2)))\n",
    "    den = stddev*sqrt(2*pi)\n",
    "\n",
    "    if den == 0: return 0\n",
    "\n",
    "    return num / den\n",
    "\n",
    "\"\"\"\n",
    "Gathers the probability each record belongs to each class. This information is submitted as a list\n",
    "of tuples, one for each class, in the format (class, probability). The probability itself is more of an estimate,\n",
    "as it ignores the probability of the class. It also takes in the summary statistics dictionary \n",
    "of the training data, as well as the testing data to determine the probability a testing \n",
    "record belongs to each class. probDist determines what probability engine is used for the records.\n",
    "\"\"\"\n",
    "def probsByClass(classStats, test_data, probDist = gaussProb):\n",
    "    finalProbList = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        tempProbList = []\n",
    "        for j in classStats:\n",
    "            classProb = 1\n",
    "            for k in range(len(test_data[i])):\n",
    "                classProb *= probDist(classStats[j][k][0],classStats[j][k][1],test_data[i][k])\n",
    "\n",
    "            tempProbList.append((j,classProb))\n",
    "\n",
    "        finalProbList.append(tempProbList)\n",
    "\n",
    "    return finalProbList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       0                       1                       2\n",
      "0     (0, 4.99423698296)  (1, 2.33155079224e-17)  (2, 5.99941686211e-25)\n",
      "1     (0, 3.23953517327)  (1, 8.73266079723e-17)  (2, 1.50235580627e-24)\n",
      "2   (0, 0.0203115834614)   (1, 1.1300194082e-19)  (2, 6.27675332261e-26)\n",
      "3  (0, 0.00188243300303)  (1, 2.49570741139e-18)  (2, 4.31083432297e-24)\n",
      "4     (0, 3.47157072771)  (1, 2.34891835379e-16)  (2, 2.85135644461e-23)\n"
     ]
    }
   ],
   "source": [
    "finalProbsList = probsByClass(classStats, test_data)\n",
    "\n",
    "#Shows the probability of each record belonging to different classes\n",
    "print(pd.DataFrame(finalProbsList[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell by looking at these first few samples, the mystery records look to all be setosa flowers. We will verify this below with our final predictions and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Saving Predictions and Visualizing Results\n",
    "\n",
    "Now that we have all of the probabilities we need, we can save these results and visualize how well our model did.\n",
    "The prediction function is actually very simple. For each record, all it does is identify what class had the highest probability and save it to a predicted_answer list. The remaining two functions are the exact same as the KNN-Classifier visualization and give us a good way to identify which model is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function is the final step of the prediction process. The probabilities list found in the previous step \n",
    "are now ranked, and the highest probability class for each record is saved to a predictions list.\n",
    "\"\"\"\n",
    "def predict(finalProbList):\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(finalProbList)):\n",
    "        bestClass = -1\n",
    "        bestProb = 0\n",
    "\n",
    "        for j in range(len(finalProbList[i])):\n",
    "            if bestProb < finalProbList[i][j][1]:\n",
    "                bestClass = finalProbList[i][j][0]\n",
    "                bestProb = finalProbList[i][j][1]\n",
    "\n",
    "        predictions.append(bestClass)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\"\"\"\n",
    "This function visualizes the results of the model. Inputs include a column of the actual labels, and\n",
    "the predicted labels. It outputs a dataframe with all the results and whether or not they were correct.\n",
    "\"\"\"\n",
    "def getResults(actual, predicted):\n",
    "    result = pd.DataFrame()\n",
    "    for i in range(len(actual)):\n",
    "        predicted[i] = target_dict[predicted[i]]\n",
    "        actual[i] = target_dict[actual[i]]\n",
    "\n",
    "    result['ACTUAL'] = actual\n",
    "    result['PREDICTED'] = predicted\n",
    "    result['CORRECT?'] = [actual[i] == predicted[i] for i in range(len(actual))]\n",
    "\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "Prints the accuracy of the dataframe. Takes a column of actual labels v. predicted labels. It gives\n",
    "the accuracy, the number of cases correctly predicted, and the number of cases incorrectly predicted.\n",
    "\"\"\"\n",
    "def getAccuracy(actual, predicted):\n",
    "    total_cases = len(actual)\n",
    "    num_correct = 0\n",
    "    num_incorrect = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            num_incorrect += 1\n",
    "\n",
    "    accuracy = (num_correct / total_cases)*100\n",
    "\n",
    "    return \"Accuracy: {0}%\\nNumber correct: {1}\\nNumber incorrect: {2}\".format(str(accuracy),str(num_correct),str(num_incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Running the Model Against Our Data\n",
    "\n",
    "Finally, we can consolidate all of these functions into a **main method**, as seen below. This will give us a quick way to call up our function with certain inputs and get our results without having to call all of our sub-functions. ***This step is unnecessary**; you could just run each step on its own. However, generally speaking this is bad practice for software development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs the entire script for the Iris dataset.\n",
    "\"\"\"\n",
    "def main(data,labels,train_test_split_ratio):\n",
    "    iris_train, test_data, test_labels = train_test_split(data, labels, train_test_split_ratio)\n",
    "    classSepDict = classSep(iris_train)\n",
    "    classStatDict = classStats(classSepDict)\n",
    "    finalProbs = probsByClass(classStatDict, test_data)\n",
    "    preds = predict(finalProbs)\n",
    "    results = getResults(test_labels,preds)\n",
    "    accuracy = getAccuracy(test_labels,preds)\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to go. Now we can run all of our code using just the **main method and our three inputs**: data, labels, and our train_test split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ACTUAL   PREDICTED CORRECT?\n",
      "0       setosa      setosa     True\n",
      "1       setosa      setosa     True\n",
      "2       setosa      setosa     True\n",
      "3       setosa      setosa     True\n",
      "4       setosa      setosa     True\n",
      "5       setosa      setosa     True\n",
      "6       setosa      setosa     True\n",
      "7       setosa      setosa     True\n",
      "8       setosa      setosa     True\n",
      "9       setosa      setosa     True\n",
      "10      setosa      setosa     True\n",
      "11      setosa      setosa     True\n",
      "12      setosa      setosa     True\n",
      "13      setosa      setosa     True\n",
      "14      setosa      setosa     True\n",
      "15      setosa      setosa     True\n",
      "16      setosa      setosa     True\n",
      "17  versicolor  versicolor     True\n",
      "18  versicolor  versicolor     True\n",
      "19  versicolor  versicolor     True\n",
      "20  versicolor  versicolor     True\n",
      "21  versicolor  versicolor     True\n",
      "22  versicolor  versicolor     True\n",
      "23  versicolor  versicolor     True\n",
      "24  versicolor  versicolor     True\n",
      "25  versicolor  versicolor     True\n",
      "26  versicolor  versicolor     True\n",
      "27  versicolor  versicolor     True\n",
      "28  versicolor  versicolor     True\n",
      "29  versicolor  versicolor     True\n",
      "30  versicolor  versicolor     True\n",
      "31   virginica   virginica     True\n",
      "32   virginica   virginica     True\n",
      "33   virginica   virginica     True\n",
      "34   virginica   virginica     True\n",
      "35   virginica   virginica     True\n",
      "36   virginica   virginica     True\n",
      "37   virginica   virginica     True\n",
      "\n",
      "Accuracy: 100.0%\n",
      "Number correct: 38\n",
      "Number incorrect: 0\n"
     ]
    }
   ],
   "source": [
    "#Execute Script\n",
    "main(iris_data,iris_label,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model appears to be **very accurate** in determining flower types. In fact, this model commonly outperforms more complicated ones, though it relies heavily on the assumption that all features are independent of one another. \n",
    "\n",
    "#### Future Exploration\n",
    "\n",
    "There are many other ways to make a Bayesian algorithm. For example, you could try to optimize it for different data sets using different probability distributions (Multnomial, poisson, etc.). Have fun with this, and please commit it so all of us can see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
