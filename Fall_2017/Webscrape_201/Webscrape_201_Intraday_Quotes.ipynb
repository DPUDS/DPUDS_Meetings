{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping 201: Gathering Minute-to-Minute Stock Data\n",
    "\n",
    "This tutorial assumes that you have at least looked through **Webscraping 101**. If you have any confusion about topics covered in this notebook please use it as a reference. \n",
    "\n",
    "Many websites and APIs will allow you to extract stock data from them. Typically this data exists as one day per row. That is, you will get an open, high, low, and close price for a company for each day. However, stock prices fluctuate constantly, and to get minute-by-minute snapshots of stock data you almost always have to pay for a(n expensive) subscription. \n",
    "\n",
    "Fortunately, webscraping can help us get around this by collecting minute-by-minute stock data for us and saving it for later.\n",
    "\n",
    "### Dependencies:\n",
    "\n",
    "The dependencies here are the same as for **Webscrape 101**, with the addition of time and datetime for scheduling purposes.\n",
    "\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [urllib.request](https://docs.python.org/3.0/library/urllib.request.html)\n",
    "* [ssl](https://docs.python.org/2/library/ssl.html)\n",
    "* [time](https://docs.python.org/3/library/time.html)\n",
    "* [datetime](https://docs.python.org/3/library/datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find a Stock Website to Scrape\n",
    "\n",
    "For this tutorial, we will be using Yahoo Finance to scrape stock data. With that said, you can use any site you want that permits scraping. By navigating to Yahoo Finance and searching for a stock (say Google), we find that the URL is found to be:\n",
    "\n",
    "https://finance.yahoo.com/quote/GOOG?p=GOOG\n",
    "\n",
    "If you notice, the URL seems to simply add the stock ticker to an existing URL string. If, say, we exchanged **GOOG** out for **AAPL**, we would find that we navigate to Apple's yahoo stock webpage. This will allow us to dynamically set the URL we want to go to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Dependencies and Set Meta-parameters\n",
    "\n",
    "Meta-parameters here mean anything that needs to be a global variable. Due to the way the following code is nested, the parameters below need to be global in scope to be recognized by the necessary functions. \n",
    "\n",
    "First we have our unverified ssl context parameter.\n",
    "\n",
    "Last, we have a dictionary of URLs that can be changed for scraping. Though we are only writing code to scrape from Yahoo, creating functions to scrape from other websites would be a great way to further explore webscraping. If you can create a function to do this, we will merge your code to the master branch for all to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "#\n",
    "# Import Dependencies and Data Collection\n",
    "#\n",
    "###########################################\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "import ssl\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "###########################################\n",
    "#\n",
    "#          Set Meta-parameters\n",
    "#\n",
    "###########################################\n",
    "\n",
    "#URl dictionary of sites that could be web-scraped\n",
    "URL_dict = {'yahoo': 'https://finance.yahoo.com/quote/STOCK?p=STOCK',\n",
    "            'NASDAQ': 'http://www.nasdaq.com/symbol/STOCK',\n",
    "            'bloomberg': 'https://www.bloomberg.com/quote/STOCK:US',\n",
    "            'reuters': 'http://www.reuters.com/finance/stocks/overview?symbol=STOCK.O'}\n",
    "\n",
    "#Set ssl context to allow for an unverified handshake with a network site\n",
    "ssl_context = ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Scraping Engine\n",
    "\n",
    "Now, let's create a function that will scrape stock price and volume data from Yahoo finance. We want to make this dynamic, so we can get data from any stock that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the yahoo ohlcv (open, high, low, close, volume)\n",
    "webscraping engine. This function will return the most \n",
    "recent stock price and volume as found on the yahoo\n",
    "finance webpage.\n",
    "\n",
    "# ticker = any stock ticker, lower or uppercase\n",
    "'''\n",
    "def yahoo_minute_ohlcv_data(ticker):\n",
    "\n",
    "    #Creates the link needed to contact yahoo finance\n",
    "    link  = URL_dict['yahoo'].replace('STOCK',ticker.lower())\n",
    "\n",
    "    #Opens the URL link created above\n",
    "    page = req.urlopen(link, context = ssl_context)\n",
    "\n",
    "    #Allows for any scraping exceptions to be caught and handled\n",
    "    try:\n",
    "        #Extracts all html data from the page opened above\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        #Digging through html to find correct tags (classes) for stock price\n",
    "        price = soup.find('span', class_= 'Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)').find(text = True)\n",
    "\n",
    "        #Refining down html tags for volume [(6th) row of the vol_table, within vol_class]\n",
    "        vol_class = soup.find('div', class_='D(ib) W(1/2) Bxz(bb) Pend(12px) Va(t) ie-7_D(i)')\n",
    "        vol_table = vol_class.findAll('td', class_= 'Ta(end) Fw(b) Lh(14px)')\n",
    "        volume = vol_table[6].find(text = True).replace(',','')\n",
    "\n",
    "        #Returns stock data and price as float values\n",
    "        price, volume = float(price), float(volume)\n",
    "\n",
    "        return price, volume\n",
    "\n",
    "    # If stock data ill-formatted, scraping attempt is skipped\n",
    "    # Sometimes data is reported as N/A briefly as it changes\n",
    "    except Exception as e:\n",
    "            print(\"\\nError in scraping data, current scrape attempt skipped\")\n",
    "            print(\"Message: %s\"%(e))\n",
    "            print(\"Re-scraping\\n\")\n",
    "            yahoo_minute_ohlcv_data(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the code above. \n",
    "\n",
    "    1. A link is created by dynamically adding the stock ticker to the URL provided by the URL_dict meta-parameter\n",
    "    2. The webpage found at the link previously created is opened, and its html contents are saved \n",
    "    3. Include a try and except clause here to ensure that failed scrape attempts do not fail the entire process\n",
    "    4. After going to the yahoo website and inspecting its HTML contents, we find the correct paths to the data \n",
    "    5. We pull the most current stock price and volume\n",
    "    6. Finally, we re-format the data to be float values and return them.\n",
    "    7. If an exception is found, a message is printed detailing the issue and the scrape attempts to pull data again\n",
    "\n",
    "\n",
    "Awesome! If we give it a run, we can see that it is indeed working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 908.14\n",
      "Volume: 1569344.0\n"
     ]
    }
   ],
   "source": [
    "ticker = 'GOOG'\n",
    "\n",
    "price, volume = yahoo_minute_ohlcv_data(ticker)\n",
    "\n",
    "print(\"Price: \" + str(price))\n",
    "print(\"Volume: \" + str(volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Automated Scrape Scheduler\n",
    "\n",
    "Now that we have a webscraping engine built properly, let's create a function that will schedule our scraper to run and save data every minute. The code can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the master function for webscraping intraday stock\n",
    "data on a minute to minute basis. Waiting until the top of \n",
    "the next minute, the get_ohlcv_data function notifies the \n",
    "user the time that the scrape starts. It then pulls stock \n",
    "price and volume data a total of five times over the course\n",
    "of a given minute. The open, high, low, close, and average\n",
    "volume are then calculated from this. Every minute, a new\n",
    "row is added with ohlcv features, as well as a timestamp.\n",
    "Once the duration (in minutes) is over, all of the collected \n",
    "data is saved as a csv file, named in the format:\n",
    "\n",
    "[*stock_ticker*]_minute_ohlcv_data_[*date*].csv\n",
    "\n",
    "# ticker - Any stock ticker\n",
    "# provider - any website provider in URL_dict\n",
    "  *NOTE* ONLY THE YAHOO SCRAPING ENGINE HAS BEEN CREATED\n",
    "# duration - number of minutes for which the user wants data\n",
    "'''\n",
    "def get_ohlcv_data(ticker, provider, duration):\n",
    "\n",
    "    #Stores list of all scraped data, one row per minute.\n",
    "    #This is what is stored as a csv eventually.\n",
    "    master_stock_data_list = []\n",
    "\n",
    "    #Creates a string with the necessary code to execute the given engine.given\n",
    "    #(e.g. 'yahoo_minute_ohlcv_data('GOOG')')\n",
    "    engine = str(provider) + '_minute_ohlcv_data(\\'' + ticker + '\\')'\n",
    "\n",
    "    #Intializing count variables for duration and scrape counts\n",
    "    scrape_count = 0\n",
    "    actual_duration = 0\n",
    "\n",
    "    #Initializing price and volume lists for intra-minute scrapes\n",
    "    price_list = []\n",
    "    vol_list = []\n",
    "\n",
    "    #Will turn to true at the top of the next minute. Allows Web-scrape pulls\n",
    "    # to be organized.\n",
    "    scrape_start = False\n",
    "\n",
    "    #Initializes all datetime values for starting the scrape\n",
    "    now = dt.datetime.now()\n",
    "    #Rounds down current time to the most recent minute\n",
    "    now = dt.datetime.strptime(now.strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "    #Adds a minute to the rounded down time\n",
    "    start = now + dt.timedelta(minutes = 1, seconds = -now.second)\n",
    "\n",
    "    #Pre-execution of web-scraping engine\n",
    "    while not scrape_start:\n",
    "\n",
    "        #If the time has reached the next minute, begin execute sequence by changing scrape_start\n",
    "        if dt.datetime.now() >= start:\n",
    "            scrape_start = True\n",
    "            next_scrape = dt.datetime.strptime(dt.datetime.now().strftime('%Y-%m-%d %H:%M'), '%Y-%m-%d %H:%M')\n",
    "            print('\\nScrape started at %s and will run for %s minute(s)\\n'%(str(dt.datetime.now())[11:-7], duration))\n",
    "\n",
    "        #Notify the user that the scraping engine is still waiting.\n",
    "        else:\n",
    "            print('Scrape starting in %s seconds'%str((start - dt.datetime.now()))[5:-7])\n",
    "            #Pauses the sequence for 1 second\n",
    "            time.sleep(1)\n",
    "\n",
    "    #Main execution loop\n",
    "    while scrape_start and actual_duration < duration:\n",
    "\n",
    "        #Resets after the scraper has pulled 5 times in one minute\n",
    "        while scrape_count < 5:\n",
    "\n",
    "            if dt.datetime.now() >= next_scrape:\n",
    "\n",
    "                #Gets price and volume data\n",
    "                price, volume = eval(engine)\n",
    "\n",
    "                #Add price and volume data to respective lists\n",
    "                price_list.append(price)\n",
    "                vol_list.append(volume)\n",
    "\n",
    "                #Amend next_scrape time to be ten seconds later (allows pause)\n",
    "                next_scrape += dt.timedelta(seconds = 10)\n",
    "\n",
    "                #Increase scrape count by one\n",
    "                scrape_count += 1\n",
    "\n",
    "        #Gather all necessary data after scraping five times. (Date + ohlcv)\n",
    "        Date_time = dt.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "        min_open = price_list[0]\n",
    "        min_high = max(price_list)\n",
    "        min_low = min(price_list)\n",
    "        min_close = price_list[len(price_list) - 1]\n",
    "        min_avg_volume = int(np.mean(vol_list))\n",
    "\n",
    "        #Create a row with all necessary data\n",
    "        row = [Date_time, min_open, min_high, min_low, min_close, min_avg_volume]\n",
    "\n",
    "        #Add row to master data list\n",
    "        master_stock_data_list.append(row)\n",
    "\n",
    "        #Notify user of the added row\n",
    "        print(\"[%s] Row Stored\"%Date_time)\n",
    "\n",
    "        #Reset time parameters, counts, and temporary lists\n",
    "        scrape_count = 0\n",
    "        next_scrape += dt.timedelta(seconds = 10)\n",
    "        price_list = []\n",
    "        vol_list = []\n",
    "        actual_duration += 1\n",
    "\n",
    "    #Notify user when the webscrape has finished\n",
    "    print(\"\\n[%s] Scrape Finished.\\n\"%(dt.datetime.now().strftime('%Y-%m-%d %H:%M')))\n",
    "\n",
    "    #Create a DataFrame of all the gathered stock data, and remove dummy index\n",
    "    stock_df = pd.DataFrame(master_stock_data_list, columns = ['DateTime', 'Open', 'High', 'Low', 'Close', \"Avg_Volume\"])\n",
    "    stock_df.set_index('DateTime', inplace = True)\n",
    "\n",
    "    #print(stock_df)\n",
    "\n",
    "    #Parameters to create file name\n",
    "    date = dt.datetime.now().strftime('%Y-%m-%d')\n",
    "    file_name = ticker.upper() + '_Intraday_Stock_Data_' + date + '.csv'\n",
    "\n",
    "    #Store stock data scrapings as a csv file\n",
    "    stock_df.to_csv(file_name, sep = ',')\n",
    "    print('File saved at location: %s'%(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Function Does \n",
    "\n",
    "Woah, that is a lot of code. Before delving into it, let's discuss what it is doing from a practical standpoint. \n",
    "\n",
    "    1. Once started, the program waits until the start of the next minute to run, notifying the user throughout\n",
    "    2. After starting, the program pulls stock data for the provided ticker every ten seconds, five times in a row\n",
    "    3. Since all of these prices and volumes are for the same minute, they are saved to a list\n",
    "    4. Once data has been pulled five times, the open, high, low, and close price for the stock is found\n",
    "    5. Items in the stock price list are chronological, making the previous step easy\n",
    "    6. An average is taken of the volume list (vol_list), and saved as the volume variable\n",
    "    7. Finally, all stock values (ohlcv) are saved to a list and stored.\n",
    "    8. After a certain number of minutes has passed (equal to duration value), the stored data is saved as a csv \n",
    "    \n",
    "### Technical Explanation\n",
    "\n",
    "To add to our practical explanation, let me add some technical color to what is going on. First, the engine variable concatenates the string equivalent to running the function with the variables we need. For example, if we were trying to get **ibm** stock prices the engine variable would equal **`'yahoo_minute_ohlcv_data('IBM')'`**. By using the **`eval`** function, we can run our scraping engine dynamically with **any stock and any media provider**. \n",
    "\n",
    "The first while loop iterates in the seconds leading up to the top of the next minute. If the program is started at 11:30:34, the loop will continue to print statements 26 times, letting the user know when the program will actually begin to start scraping data.\n",
    "\n",
    "Next, when scrape_start is set to **`True`** and the actual_duration is less than the planned duration set in the parameters, the scraping engine will be run every ten seconds, up to five times. These values are stored in lists, which are then used to identify high, low, open, and close prices. Finally, this data is added to the **`master_stock_data_list`** variable. \n",
    "\n",
    "Once the scraping process is finished. The data is fed into a pandas DataFrame and saved as a csv using the file name:\n",
    "\n",
    "[*stock_ticker*]_minute_ohlcv_data_[*date*].csv_\n",
    "\n",
    "Now that we have our code created, **let's make a main method and run our code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Main method. All variables are defined as they were for get_ohlcv_data.\n",
    "Only new addition is changing the working directory.\n",
    "'''\n",
    "def main(ticker, provider, duration):\n",
    "\n",
    "    #Change this to your current working directory of choice\n",
    "    os.chdir('/Users/Sam/Documents/Python/DPUDS/DPUDS_Meetings/Fall_2017/Webscrape_201')\n",
    "\n",
    "    get_ohlcv_data(ticker, provider, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We are now ready to see what our scraping automator does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape starting in 05 seconds\n",
      "Scrape starting in 04 seconds\n",
      "Scrape starting in 03 seconds\n",
      "Scrape starting in 02 seconds\n",
      "Scrape starting in 01 seconds\n",
      "Scrape starting in 00 seconds\n",
      "\n",
      "Scrape started at 15:58:00 and will run for 5 minute(s)\n",
      "\n",
      "[2017-08-10 15:58] Row Stored\n",
      "[2017-08-10 15:59] Row Stored\n",
      "[2017-08-10 16:00] Row Stored\n",
      "[2017-08-10 16:01] Row Stored\n",
      "[2017-08-10 16:02] Row Stored\n",
      "\n",
      "[2017-08-10 16:02] Scrape Finished.\n",
      "\n",
      "File saved at location: /Users/Sam/Documents/Python/DPUDS/DPUDS_Meetings/Fall_2017/Webscrape_Stock_Data\n"
     ]
    }
   ],
   "source": [
    "ticker = 'ibm'\n",
    "\n",
    "main(ticker,'yahoo',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Final Thoughts\n",
    "\n",
    "By following the file path printed out, we can actually view the data we collected. Below is a data sample collected earlier for Tesla. \n",
    "\n",
    "The scraper ran for 90 minutes, but in theory it could run the entire day (just don't let your computer fall asleep or the operation will terminate and you will lose your data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DateTime      Open      High       Low     Close  Avg_Volume\n",
      "0   2017-08-11 13:18  357.5353  357.6000  357.4200  357.4200     2905183\n",
      "1   2017-08-11 13:19  357.5390  357.6100  357.4000  357.4000     2910566\n",
      "2   2017-08-11 13:20  357.4300  357.4300  357.3612  357.3612     2915401\n",
      "3   2017-08-11 13:21  357.3794  357.3794  357.0000  357.0000     2922222\n",
      "4   2017-08-11 13:22  357.0000  357.2600  356.6900  356.6900     2930593\n",
      "5   2017-08-11 13:23  356.7200  356.7200  356.7059  356.7059     2935380\n",
      "6   2017-08-11 13:24  356.7900  356.7900  356.7900  356.7900     2937465\n",
      "7   2017-08-11 13:25  356.9636  356.9636  356.7200  356.7600     2938731\n",
      "8   2017-08-11 13:26  356.9900  357.0341  356.9900  357.0300     2941034\n",
      "9   2017-08-11 13:27  356.9100  356.9100  356.8900  356.8900     2942831\n",
      "10  2017-08-11 13:28  357.1500  357.1500  356.7900  356.7900     2958126\n",
      "11  2017-08-11 13:29  356.7000  356.7200  356.6625  356.6625     2970375\n",
      "12  2017-08-11 13:30  356.6800  356.8200  356.6800  356.8100     2974341\n",
      "13  2017-08-11 13:31  356.8000  357.0500  356.8000  356.8932     2979685\n",
      "14  2017-08-11 13:32  356.8400  357.1200  356.8400  357.0400     2985717\n",
      "15  2017-08-11 13:33  357.0716  357.0716  356.8400  356.8400     2989636\n",
      "16  2017-08-11 13:34  356.8600  357.2090  356.8600  357.2090     2995195\n",
      "17  2017-08-11 13:35  357.0100  357.0100  357.0100  357.0100     2998919\n",
      "18  2017-08-11 13:36  357.1675  357.2200  357.1200  357.2200     3006252\n",
      "19  2017-08-11 13:37  357.0250  357.1400  357.0250  357.0500     3011377\n",
      "20  2017-08-11 13:38  357.1500  357.1500  357.1500  357.1500     3018680\n",
      "21  2017-08-11 13:39  357.2500  357.3326  357.1300  357.1300     3025277\n",
      "22  2017-08-11 13:40  357.1300  357.4600  357.1300  357.4600     3030169\n",
      "23  2017-08-11 13:41  357.3700  357.4000  357.2800  357.2800     3040409\n",
      "24  2017-08-11 13:42  357.4900  357.4900  357.3001  357.4325     3045805\n",
      "25  2017-08-11 13:43  357.3900  357.4016  357.3400  357.3400     3048230\n",
      "26  2017-08-11 13:44  357.3400  357.4190  357.3400  357.4190     3049202\n",
      "27  2017-08-11 13:45  357.3100  357.3900  357.2500  357.2500     3051254\n",
      "28  2017-08-11 13:46  357.4000  357.4400  357.3900  357.4400     3056476\n",
      "29  2017-08-11 13:47  357.3000  357.5000  357.2400  357.5000     3060523\n",
      "..               ...       ...       ...       ...       ...         ...\n",
      "60  2017-08-11 14:18  357.8900  357.9200  357.7400  357.8500     3222559\n",
      "61  2017-08-11 14:19  357.9500  357.9500  357.7895  357.8500     3224570\n",
      "62  2017-08-11 14:20  358.1900  358.1900  358.0191  358.1100     3229079\n",
      "63  2017-08-11 14:21  357.7800  358.0899  357.7800  358.0899     3231920\n",
      "64  2017-08-11 14:22  358.0900  358.1000  358.0699  358.1000     3233329\n",
      "65  2017-08-11 14:23  357.9500  358.1499  357.9484  358.1499     3235852\n",
      "66  2017-08-11 14:24  358.1000  358.1510  358.1000  358.1400     3239859\n",
      "67  2017-08-11 14:25  358.1100  358.2200  358.1100  358.2200     3241664\n",
      "68  2017-08-11 14:26  358.1300  358.1395  357.9300  357.9900     3248916\n",
      "69  2017-08-11 14:27  357.9300  357.9900  357.7679  357.8700     3253710\n",
      "70  2017-08-11 14:28  357.6683  357.8400  357.6683  357.8400     3256281\n",
      "71  2017-08-11 14:29  357.6200  357.8000  357.6200  357.7300     3259509\n",
      "72  2017-08-11 14:30  357.7300  357.7300  357.6900  357.6900     3261238\n",
      "73  2017-08-11 14:31  357.6900  357.8800  357.6900  357.7900     3262621\n",
      "74  2017-08-11 14:32  357.5038  357.7000  357.4500  357.4500     3276238\n",
      "75  2017-08-11 14:33  357.3000  357.3500  356.9840  356.9840     3287863\n",
      "76  2017-08-11 14:34  356.9800  357.1200  356.8100  356.8100     3298229\n",
      "77  2017-08-11 14:35  356.8000  356.8000  356.4000  356.5700     3306495\n",
      "78  2017-08-11 14:36  356.2900  356.4175  355.8600  355.8600     3319931\n",
      "79  2017-08-11 14:37  355.7700  356.1500  355.7510  356.1500     3342917\n",
      "80  2017-08-11 14:38  355.9300  356.0782  355.9200  355.9200     3349917\n",
      "81  2017-08-11 14:39  356.0200  356.3200  356.0200  356.0200     3356063\n",
      "82  2017-08-11 14:40  356.0560  356.9100  356.0560  356.7900     3366819\n",
      "83  2017-08-11 14:41  356.7100  356.7420  356.7100  356.7265     3378655\n",
      "84  2017-08-11 14:42  356.7000  356.9899  356.7000  356.9899     3382789\n",
      "85  2017-08-11 14:43  357.0000  357.1000  356.8875  356.8875     3396113\n",
      "86  2017-08-11 14:44  356.9400  356.9400  356.6076  356.6076     3406054\n",
      "87  2017-08-11 14:45  356.7725  357.1500  356.7725  357.1500     3419818\n",
      "88  2017-08-11 14:46  357.1150  357.1800  357.0200  357.1800     3428768\n",
      "89  2017-08-11 14:47  357.2429  357.2429  357.0500  357.2400     3441034\n",
      "\n",
      "[90 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "tsla_stock_df = pd.read_csv('TSLA_Intraday_Stock_Data_2017-08-11.csv')\n",
    "\n",
    "print(tsla_stock_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration\n",
    "\n",
    "If you are looking to gain more experience in this area, here are a few things that could make this program even better (and way cooler). These increase in difficulty as you move down. Let us know if you have questions!\n",
    "\n",
    "     1. Since this notebook was created, the html code of yahoo changed. \n",
    "        The code in this notebook needs to be tweaked to work again. \n",
    "        The raw code has these changes and works, but try to investigate and fix the code on your own.\n",
    "     2. Create scraping functions for other (non-yahoo) URLs listed in the URL dictionary meta-parameter\n",
    "     3. Find a way to have the program send you a text / email when the scrape is finished\n",
    "     4. Find a way for the program to run while the computer sleeps\n",
    "     5. Find a way to have the scraper start at market open and run until market close automatically\n",
    "     6. Find a way to have the scraper scrape multiple stocks at the same time, in parallel\n",
    "     7. Find a way to make the scraper scrape a week straight, saving to the same file, but only during market hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
