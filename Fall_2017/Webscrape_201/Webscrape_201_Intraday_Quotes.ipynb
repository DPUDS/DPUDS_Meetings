{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping 201: Gathering Minute-to-Minute Stock Data\n",
    "\n",
    "This tutorial assumes that you have at least looked through **Webscraping 101**. If you have any confusion about topics covered in this notebook please use it as a reference. \n",
    "\n",
    "Many websites and APIs will allow you to extract stock data from them. Typically this data exists as one day per row. That is, you will get an open, high, low, and close price for a company for each day. However, stock prices fluctuate constantly, and to get minute-by-minute snapshots of stock data you almost always have to pay for a(n expensive) subscription. \n",
    "\n",
    "Fortunately, webscraping can help us get around this by collecting minute-by-minute stock data for us and saving it for later.\n",
    "\n",
    "### Dependencies:\n",
    "\n",
    "The dependencies here are the same as for Webscrape 101, with the addition of time and datetime for scheduling purposes.\n",
    "\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [urllib.request](https://docs.python.org/3.0/library/urllib.request.html)\n",
    "* [ssl](https://docs.python.org/2/library/ssl.html)\n",
    "* [time](https://docs.python.org/3/library/time.html)\n",
    "* [datetime](https://docs.python.org/3/library/datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find a Stock Website to Scrape\n",
    "\n",
    "For this tutorial, we will be using Yahoo Finance to scrape stock data. With that said, you can use any site you wanted that allows scraping. By navigating to yahoo finance and searching for a stock (say Google), we find that the URL is found to be:\n",
    "\n",
    "https://finance.yahoo.com/quote/GOOG?p=GOOG\n",
    "\n",
    "If you notice, the URL looks to simply add the stock ticker to an existing URL string. If, say, we exchanged **GOOG** out for **AAPL**, we would find that we navigate to Apple's yahoo stock webpage. This will allow us to dynamically set the URL we want to go to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Dependencies and Set Meta-parameters\n",
    "\n",
    "Meta-parameters here mean anything that needs to be a global variable. Due to the way the following code is nested, the parameters below need to be global in scope to be recognized by the necessary functions. \n",
    "\n",
    "First we have our unverified ssl context parameter.\n",
    "\n",
    "Last, we have a dictionary of URLs that can be changed for scraping. Though we are only writing code to scrape from Yahoo, creating functions to scrape from other websites would be a great way to further explore webscraping. If you can create a function to do this, we will merge your code to the master branch for all to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "#\n",
    "# Import Dependencies and Data Collection\n",
    "#\n",
    "###########################################\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req \n",
    "import ssl\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "###########################################\n",
    "#\n",
    "#          Set Meta-parameters\n",
    "#\n",
    "###########################################\n",
    "\n",
    "#URl dictionary of sites that could be web-scraped\n",
    "URL_dict = {'yahoo': 'https://finance.yahoo.com/quote/STOCK?p=STOCK',\n",
    "            'NASDAQ': 'http://www.nasdaq.com/symbol/STOCK',\n",
    "            'bloomberg': 'https://www.bloomberg.com/quote/STOCK:US',\n",
    "            'reuters': 'http://www.reuters.com/finance/stocks/overview?symbol=STOCK.O'}\n",
    "\n",
    "#Set ssl context to allow for an unverified handshake with a network site\n",
    "ssl_context = ssl._create_unverified_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Scraping Engine\n",
    "\n",
    "Now, let's create a function that will scrape stock price and volume data from Yahoo finance. We want to make this dynamic, so we can get data from any stock that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the yahoo ohlcv (open, high, low, close, volume)\n",
    "webscraping engine. This function will return the most \n",
    "recent stock price and volume as found on the yahoo\n",
    "finance webpage.\n",
    "\n",
    "# ticker = any stock ticker, lower or uppercase\n",
    "'''\n",
    "def yahoo_minute_ohlcv_data(ticker):\n",
    "\n",
    "    #Creates the link needed to contact yahoo finance\n",
    "    link  = URL_dict['yahoo'].replace('STOCK',ticker.lower())\n",
    "\n",
    "    #Opens the URL link created above\n",
    "    page = req.urlopen(link, context = ssl_context)\n",
    "\n",
    "    #Allows for any scraping exceptions to be caught and handled\n",
    "    try:\n",
    "        #Extracts all html data from the page opened above\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        #Digging through html to find correct tags (classes) for stock price\n",
    "        price = soup.find('span', class_= 'Trsdu(0.3s) Fw(b) Fz(36px) Mb(-4px) D(ib)').find(text = True)\n",
    "\n",
    "        #Refining down html tags for volume [(6th) row of the vol_table, within vol_class]\n",
    "        vol_class = soup.find('div', class_='D(ib) W(1/2) Bxz(bb) Pend(12px) Va(t) ie-7_D(i)')\n",
    "        vol_table = vol_class.findAll('td', class_= 'Ta(end) Fw(b) Lh(14px)')\n",
    "        volume = vol_table[6].find(text = True).replace(',','')\n",
    "\n",
    "        #Returns stock data and price as float values\n",
    "        price, volume = float(price), float(volume)\n",
    "\n",
    "        return price, volume\n",
    "\n",
    "    # If stock data ill-formatted, scraping attempt is skipped\n",
    "    # Sometimes data is reported as N/A briefly as it changes\n",
    "    except Exception as e:\n",
    "            print(\"\\nError in scraping data, current scrape attempt skipped\")\n",
    "            print(\"Message: %s\"%(e))\n",
    "            print(\"Re-scraping\\n\")\n",
    "            yahoo_minute_ohlcv_data(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the code above. \n",
    "\n",
    "    1. A link is created by dynamically adding the stock ticker to the URL provided by the URL_dict meta-parameter\n",
    "    2. The webpage found at the link previously created is opened, and its html contents are saved \n",
    "    3. Include a try and except clause here to ensure that failed scrape attempts do not fail the entire process\n",
    "    4. After going to the yahoo website and inspecting its HTML contents, we find the correct paths to the data \n",
    "    5. We pull the most current stock price and volume\n",
    "    6. Finally, we re-format the data to be float values and return them.\n",
    "    7. If an exception is found, a message is printed detailing the issue and the scrape attempts to pull data again\n",
    "\n",
    "\n",
    "Awesome! If we give it a run, we can see that it is indeed working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 908.14\n",
      "Volume: 1569344.0\n"
     ]
    }
   ],
   "source": [
    "ticker = 'GOOG'\n",
    "\n",
    "price, volume = yahoo_minute_ohlcv_data(ticker)\n",
    "\n",
    "print(\"Price: \" + str(price))\n",
    "print(\"Volume: \" + str(volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Automated Scrape Scheduler\n",
    "\n",
    "Now that we have a webscraping engine built properly, let's create a function that will schedule our scraper to run and save data every minute. The code can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the master function for webscraping intraday stock\n",
    "data on a minute to minute basis. Waiting until the top of \n",
    "the next minute, the get_ohlcv_data function notifies the \n",
    "user the time that the scrape starts. It then pulls stock \n",
    "price and volume data a total of five times over the course\n",
    "of a given minute. The open, high, low, close, and average\n",
    "volume are then calculated from this. Every minute, a new\n",
    "row is added with ohlcv features, as well as a timestamp.\n",
    "Once the duration (in minutes) is over, all of the collected \n",
    "data is saved as a csv file, named in the format:\n",
    "\n",
    "[*stock_ticker*]_minute_ohlcv_data_[*date*].csv\n",
    "\n",
    "# ticker - Any stock ticker\n",
    "# provider - any website provider in URL_dict\n",
    "  *NOTE* ONLY THE YAHOO SCRAPING ENGINE HAS BEEN CREATED\n",
    "# duration - number of minutes for which the user wants data\n",
    "'''\n",
    "def get_ohlcv_data(ticker, provider, duration):\n",
    "\n",
    "    #Stores list of all scraped data, one row per minute.\n",
    "    #This is what is stored as a csv eventually.\n",
    "    master_stock_data_list = []\n",
    "\n",
    "    #Creates a string with the necessary code to execute the given engine.given\n",
    "    #(e.g. 'yahoo_minute_ohlcv_data('GOOG')')\n",
    "    engine = str(provider) + '_minute_ohlcv_data(\\'' + ticker + '\\')'\n",
    "\n",
    "    #Intializing count variables for duration and scrape counts\n",
    "    scrape_count = 0\n",
    "    actual_duration = 0\n",
    "\n",
    "    #Initializing price and volume lists for intra-minute scrapes\n",
    "    price_list = []\n",
    "    vol_list = []\n",
    "\n",
    "    #Will turn to true at the top of the next minute. Allows Web-scrape pulls\n",
    "    # to be organized.\n",
    "    scrape_start = False\n",
    "\n",
    "    #Initializes all datetime values for starting the scrape\n",
    "    now = dt.datetime.now()\n",
    "    #Rounds down current time to the most recent minute\n",
    "    now = dt.datetime.strptime(now.strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "    #Adds a minute to the rounded down time\n",
    "    start = now + dt.timedelta(minutes = 1, seconds = -now.second)\n",
    "\n",
    "    #Pre-execution of web-scraping engine\n",
    "    while not scrape_start:\n",
    "\n",
    "        #If the time has reached the next minute, begin execute sequence by changing scrape_start\n",
    "        if dt.datetime.now() >= start:\n",
    "            scrape_start = True\n",
    "            next_scrape = dt.datetime.strptime(dt.datetime.now().strftime('%Y-%m-%d %H:%M'), '%Y-%m-%d %H:%M')\n",
    "            print('\\nScrape started at %s and will run for %s minute(s)\\n'%(str(dt.datetime.now())[11:-7], duration))\n",
    "\n",
    "        #Notify the user that the scraping engine is still waiting.\n",
    "        else:\n",
    "            print('Scrape starting in %s seconds'%str((start - dt.datetime.now()))[5:-7])\n",
    "            #Pauses the sequence for 1 second\n",
    "            time.sleep(1)\n",
    "\n",
    "    #Main execution loop\n",
    "    while scrape_start and actual_duration < duration:\n",
    "\n",
    "        #Resets after the scraper has pulled 5 times in one minute\n",
    "        while scrape_count < 5:\n",
    "\n",
    "            if dt.datetime.now() >= next_scrape:\n",
    "\n",
    "                #Gets price and volume data\n",
    "                price, volume = eval(engine)\n",
    "\n",
    "                #Add price and volume data to respective lists\n",
    "                price_list.append(price)\n",
    "                vol_list.append(volume)\n",
    "\n",
    "                #Amend next_scrape time to be ten seconds later (allows pause)\n",
    "                next_scrape += dt.timedelta(seconds = 10)\n",
    "\n",
    "                #Increase scrape count by one\n",
    "                scrape_count += 1\n",
    "\n",
    "        #Gather all necessary data after scraping five times. (Date + ohlcv)\n",
    "        Date_time = dt.datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "        min_open = price_list[0]\n",
    "        min_high = max(price_list)\n",
    "        min_low = min(price_list)\n",
    "        min_close = price_list[len(price_list) - 1]\n",
    "        min_avg_volume = int(np.mean(vol_list))\n",
    "\n",
    "        #Create a row with all necessary data\n",
    "        row = [Date_time, min_open, min_high, min_low, min_close, min_avg_volume]\n",
    "\n",
    "        #Add row to master data list\n",
    "        master_stock_data_list.append(row)\n",
    "\n",
    "        #Notify user of the added row\n",
    "        print(\"[%s] Row Stored\"%Date_time)\n",
    "\n",
    "        #Reset time parameters, counts, and temporary lists\n",
    "        scrape_count = 0\n",
    "        next_scrape += dt.timedelta(seconds = 10)\n",
    "        price_list = []\n",
    "        vol_list = []\n",
    "        actual_duration += 1\n",
    "\n",
    "    #Notify user when the webscrape has finished\n",
    "    print(\"\\n[%s] Scrape Finished.\\n\"%(dt.datetime.now().strftime('%Y-%m-%d %H:%M')))\n",
    "\n",
    "    #Create a DataFrame of all the gathered stock data, and remove dummy index\n",
    "    stock_df = pd.DataFrame(master_stock_data_list, columns = ['DateTime', 'Open', 'High', 'Low', 'Close', \"Avg_Volume\"])\n",
    "    stock_df.set_index('DateTime', inplace = True)\n",
    "\n",
    "    #print(stock_df)\n",
    "\n",
    "    #Parameters to create file name\n",
    "    date = dt.datetime.now().strftime('%Y-%m-%d')\n",
    "    file_name = ticker.upper() + '_Intraday_Stock_Data_' + date + '.csv'\n",
    "\n",
    "    #Store stock data scrapings as a csv file\n",
    "    stock_df.to_csv(file_name, sep = ',')\n",
    "    print('File saved at location: %s'%(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Function Does \n",
    "\n",
    "Woah, that is a lot of code. Before delving into it, let's discuss what it is doing from a practical standpoint. \n",
    "\n",
    "    1. Once started, the program waits until the start of the next minute to run, notifying the user throughout\n",
    "    2. After starting, the program pulls stock data for the provided ticker every ten seconds, five times in a row\n",
    "    3. Since all of these prices and volumes are for the same minute, they are saved to a list\n",
    "    4. Once data has been pulled five times, the open, high, low, and close price for the stock is found\n",
    "    5. Items in the stock price list are chronological, making the previous step easy\n",
    "    6. An average is taken of the volume list (vol_list), and saved as the volume variable\n",
    "    7. Finally, all stock values (ohlcv) are saved to a list and stored.\n",
    "    8. After a certain number of minutes has passed (equal to duration value), the stored data is saved as a csv \n",
    "    \n",
    "### Technical Explanation\n",
    "\n",
    "To add to our practical explanation, let me add some technical color to what is going on. First, the engine variable concatenates the string equivalent to running the function with the variables we need. For example, if we were trying to get **ibm** stock prices the engine variable would equal **`'yahoo_minute_ohlcv_data('IBM')'`**. By using the **`eval`** function, we can run our scraping engine dynamically with **any stock and any media provider**. \n",
    "\n",
    "The first while loop iterates in the seconds leading up to the top of the next minute. If the program is started at 11:30:34, the loop will continue to print statements 26 times, letting the user know when the program will actually begin to start scraping data.\n",
    "\n",
    "Next, when scrape_start is set to **`True`** and the actual_duration is less than the planned duration set in the parameters, the scraping engine will be run every ten seconds, up to five times. These values are stored in lists, which are then used to identify high, low, open, and close prices. Finally, this data is added to the **`master_stock_data_list`** variable. \n",
    "\n",
    "Once the scraping process is finished. The data is fed into a pandas DataFrame and saved as a csv using the file name:\n",
    "\n",
    "_[*stock_ticker*]_minute_ohlcv_data_[*date*].csv_\n",
    "\n",
    "Now that we have our code created, **let's make a main method and run our code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Main method. All variables are defined as they were for get_ohlcv_data.\n",
    "Only new addition is changing the working directory.\n",
    "'''\n",
    "def main(ticker, provider, duration):\n",
    "\n",
    "    #Change this to your current working directory of choice\n",
    "    os.chdir('/Users/Sam/Documents/Python/DPUDS/DPUDS_Meetings/Fall_2017/Webscrape_Stock_Data')\n",
    "\n",
    "    get_ohlcv_data(ticker, provider, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We are now ready to see what our scraping automator does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape starting in 05 seconds\n",
      "Scrape starting in 04 seconds\n",
      "Scrape starting in 03 seconds\n",
      "Scrape starting in 02 seconds\n",
      "Scrape starting in 01 seconds\n",
      "Scrape starting in 00 seconds\n",
      "\n",
      "Scrape started at 15:58:00 and will run for 5 minute(s)\n",
      "\n",
      "[2017-08-10 15:58] Row Stored\n",
      "[2017-08-10 15:59] Row Stored\n",
      "[2017-08-10 16:00] Row Stored\n",
      "[2017-08-10 16:01] Row Stored\n",
      "[2017-08-10 16:02] Row Stored\n",
      "\n",
      "[2017-08-10 16:02] Scrape Finished.\n",
      "\n",
      "File saved at location: /Users/Sam/Documents/Python/DPUDS/DPUDS_Meetings/Fall_2017/Webscrape_Stock_Data\n"
     ]
    }
   ],
   "source": [
    "ticker = 'ibm'\n",
    "\n",
    "main(ticker,'yahoo',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Final Thoughts\n",
    "\n",
    "By following the file path printed out, we can actually view the data we collected. Below is a data sample collected earlier for AAPL. \n",
    "\n",
    "The scraper ran for half an hour, but in theory could run the entire day (just don't let your computer fall asleep or the operation will terminate and you will lose your data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DateTime      Open      High       Low     Close  Avg_Volume\n",
      "0   2017-08-10 13:11  156.9328  156.9400  156.8900  156.9000    22408051\n",
      "1   2017-08-10 13:12  156.8900  156.9400  156.8900  156.9400    22452397\n",
      "2   2017-08-10 13:13  156.9090  156.9500  156.8700  156.8700    22486582\n",
      "3   2017-08-10 13:14  156.8000  156.8000  156.7800  156.7890    22536752\n",
      "4   2017-08-10 13:15  156.8200  156.8200  156.7200  156.7200    22592822\n",
      "5   2017-08-10 13:16  156.7000  156.7672  156.7000  156.7672    22675206\n",
      "6   2017-08-10 13:17  156.7520  156.7520  156.7000  156.7100    22717252\n",
      "7   2017-08-10 13:18  156.7493  156.8400  156.7493  156.8400    22739158\n",
      "8   2017-08-10 13:19  156.8600  156.9200  156.8600  156.9200    22777656\n",
      "9   2017-08-10 13:20  156.8900  156.9000  156.8699  156.8800    22817418\n",
      "10  2017-08-10 13:21  156.9200  157.0000  156.9200  156.9400    22861093\n",
      "11  2017-08-10 13:22  156.9400  157.0784  156.9400  157.0784    22917382\n",
      "12  2017-08-10 13:23  157.1000  157.1000  157.0547  157.0547    22988173\n",
      "13  2017-08-10 13:24  157.1300  157.2100  157.1300  157.1999    23055036\n",
      "14  2017-08-10 13:25  157.1299  157.1728  157.1299  157.1537    23134927\n",
      "15  2017-08-10 13:26  157.1801  157.2500  157.1800  157.1800    23200449\n",
      "16  2017-08-10 13:27  157.1500  157.1800  157.1200  157.1400    23281525\n",
      "17  2017-08-10 13:28  157.1500  157.2311  157.1500  157.2311    23312775\n",
      "18  2017-08-10 13:29  157.2900  157.3699  157.2900  157.3699    23383907\n",
      "19  2017-08-10 13:30  157.3600  157.4500  157.3600  157.4300    23449648\n",
      "20  2017-08-10 13:31  157.3706  157.4054  157.3400  157.3400    23516365\n",
      "21  2017-08-10 13:32  157.3400  157.3500  157.3300  157.3300    23563183\n",
      "22  2017-08-10 13:33  157.3400  157.3400  157.3099  157.3100    23601479\n",
      "23  2017-08-10 13:34  157.3147  157.3700  157.3147  157.3564    23644651\n",
      "24  2017-08-10 13:35  157.3100  157.3300  157.2100  157.2100    23720960\n",
      "25  2017-08-10 13:36  157.2600  157.2600  157.1600  157.1600    23811170\n",
      "26  2017-08-10 13:37  157.1255  157.1500  157.1255  157.1400    23863478\n",
      "27  2017-08-10 13:38  157.1400  157.1500  157.1200  157.1200    23887201\n",
      "28  2017-08-10 13:39  157.1200  157.1700  157.1200  157.1500    23927651\n",
      "29  2017-08-10 13:40  157.2050  157.2200  157.1000  157.1000    23963417\n"
     ]
    }
   ],
   "source": [
    "aapl_stock_df = pd.read_csv('AAPL_Intraday_Stock_Data_2017-08-10.csv')\n",
    "\n",
    "print(aapl_stock_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration\n",
    "\n",
    "If you are looking to gain more experience in this area, here are a few things that could make this program even better (and way cooler). These increase in difficulty as you move down. Let us know if you have questions!\n",
    "\n",
    "     1. Create scraping functions for other (non-yahoo) URLs listed in the URL dictionary meta-parameter\n",
    "     2. Find a way to have the program send you a text / email when the scrape is finished\n",
    "     3. Find a way for the program to run while the computer sleeps\n",
    "     4. Find a way to have the scraper start at market open and run until market close automatically\n",
    "     5. Find a way to have the scraper scrape multiple stocks at the same time, in parallel\n",
    "     6. Find a way to make the scraper scrape a week straight, saving to the same file, but only during market hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
